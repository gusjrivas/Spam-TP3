{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "ae42b5cdc085ad0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.120115Z",
     "start_time": "2024-04-11T15:52:15.893237Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scikit-learn nos ofrece una variedad ampliada de modelos Naive Bayes, para este problema usamos MultinomialNB que es pensado para este tipo de problemas\n",
    "from sklearn.naive_bayes import MultinomialNB   \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd4f97a897731c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# TP3: Detector de SPAM\n",
    "\n",
    "Uno de los problemas más comunes en la clasificación es la detección de correos electrónicos SPAM. Uno de los primeros modelos utilizados para abordar este problema fue el clasificador de Bayes ingenuo. La detección de SPAM es un problema persistente en el mundo digital, ya que los spammers continúan adaptando sus estrategias para eludir los filtros de correo no deseado. Además del clasificador de Bayes ingenuo, se han desarrollado y utilizado una variedad de técnicas más avanzadas en la detección de SPAM, que incluyen algoritmos de aprendizaje automático, redes neuronales y métodos basados en reglas.\n",
    "\n",
    "En este trabajo práctico, utilizaremos un conjunto de datos que consta de 4601 observaciones de correos electrónicos, de los cuales 2788 son correos legítimos y 1813 son correos SPAM. Dado que el contenido de los correos electrónicos es un tipo de dato no estructurado, es necesario procesarlo de alguna manera. Para este conjunto de datos, ya se ha aplicado un procesamiento típico en el Procesamiento del Lenguaje Natural (NLP), que consiste en contar la frecuencia de palabras observadas en los correos.\n",
    "\n",
    "El procesamiento de lenguaje natural (NLP) desempeña un papel fundamental en la detección de SPAM, ya que permite analizar el contenido de los correos electrónicos y extraer características relevantes para la clasificación. Además de contar la frecuencia de palabras, se pueden utilizar técnicas más sofisticadas, como la extracción de características semánticas y el análisis de sentimientos, para mejorar la precisión de los modelos de detección de SPAM.\n",
    "\n",
    "En este proceso, se cuenta la cantidad de ocurrencias de cada palabra en los diferentes correos.\n",
    "\n",
    "![spam counter](./spam.png)\n",
    "\n",
    "Con el fin de preservar la privacidad de los mensajes, la frecuencia de palabras se encuentra normalizada. El conjunto de datos está compuesto por 54 columnas de atributos que se denominan:\n",
    "\n",
    "- `word_freq_XXXX`: Donde `XXXX` es la palabra o símbolo. Los valores son enteros que van de 0 a 20k.\n",
    "\n",
    "Además, hay una columna adicional llamada `spam`, que es 1 si el correo es SPAM o 0 si no lo es.\n",
    "\n",
    "Los clasificadores de Bayes ingenuos fueron los primeros filtros utilizados por las aplicaciones de correo electrónico, basados en este principio de palabras. La idea es que, partiendo de un dato a priori sobre la probabilidad de que un correo sea SPAM o no, ciertas palabras nos indicarán que la probabilidad a posteriori, dadas esas palabras, es más probable que el correo sea SPAM o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.143226Z",
     "start_time": "2024-04-11T15:52:16.120715Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.671</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.450</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.022</td>\n",
       "      <td>9.744</td>\n",
       "      <td>445</td>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.729</td>\n",
       "      <td>43</td>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "5            0.00               0.00           0.00           0.0   \n",
       "6            0.00               0.00           0.00           0.0   \n",
       "7            0.00               0.00           0.00           0.0   \n",
       "8            0.15               0.00           0.46           0.0   \n",
       "9            0.06               0.12           0.77           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "5           1.85            0.00              0.00                1.85   \n",
       "6           1.92            0.00              0.00                0.00   \n",
       "7           1.88            0.00              0.00                1.88   \n",
       "8           0.61            0.00              0.30                0.00   \n",
       "9           0.19            0.32              0.38                0.00   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "5             0.00            0.00  ...         0.00        0.223   \n",
       "6             0.00            0.64  ...         0.00        0.054   \n",
       "7             0.00            0.00  ...         0.00        0.206   \n",
       "8             0.92            0.76  ...         0.00        0.271   \n",
       "9             0.06            0.00  ...         0.04        0.030   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "5          0.0        0.000        0.000        0.000   \n",
       "6          0.0        0.164        0.054        0.000   \n",
       "7          0.0        0.000        0.000        0.000   \n",
       "8          0.0        0.181        0.203        0.022   \n",
       "9          0.0        0.244        0.081        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "5                       3.000                          15   \n",
       "6                       1.671                           4   \n",
       "7                       2.450                          11   \n",
       "8                       9.744                         445   \n",
       "9                       1.729                          43   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "5                        54     1  \n",
       "6                       112     1  \n",
       "7                        49     1  \n",
       "8                      1257     1  \n",
       "9                       749     1  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"spambase.csv\") # cargando los datos desde un CSV\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5762c49a3369dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Para obtener las palábras más usadas podemos hacer un `groupby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "aa93f67db28da6ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.147047Z",
     "start_time": "2024-04-11T15:52:16.143891Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "column_sum = dataset.groupby(by=\"spam\", as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab4e7c21dd0461",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Y despues se pueden combinar las columnas en usando [pd.melt](https://pandas.pydata.org/docs/reference/api/pandas.melt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "a05390e27958e98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.150961Z",
     "start_time": "2024-04-11T15:52:16.147743Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos los atributos y target\n",
    "X = (dataset.drop(columns=\"spam\") * 100).astype(int)\n",
    "#X = dataset2.drop(columns=\"spam\")\n",
    "y = dataset[\"spam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686665f4a0ce53e5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Se separa el dataset en entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "a05dd9a86a0270d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.155862Z",
     "start_time": "2024-04-11T15:52:16.151805Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deca5d28cadd43b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Escalamos para aplicar en regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "712e38ef8afee400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.162957Z",
     "start_time": "2024-04-11T15:52:16.156768Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Lo transformamos en DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71beeba0",
   "metadata": {},
   "source": [
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "76a9e6364b4d74b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T15:52:16.168004Z",
     "start_time": "2024-04-11T15:52:16.164460Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam:\n",
      " capital_run_length_total      853233.000\n",
      "capital_run_length_longest    189265.000\n",
      "capital_run_length_average     17258.246\n",
      "word_freq_you                   4105.610\n",
      "word_freq_your                  2502.610\n",
      "word_freq_will                   997.100\n",
      "word_freq_free                   939.790\n",
      "word_freq_our                    931.800\n",
      "char_freq_!                      931.361\n",
      "word_freq_all                    732.080\n",
      "Name: 1, dtype: float64\n",
      "No Spam:\n",
      " capital_run_length_total      450181.000\n",
      "capital_run_length_longest     50782.000\n",
      "capital_run_length_average      6627.915\n",
      "word_freq_you                   3541.710\n",
      "word_freq_george                3527.560\n",
      "word_freq_hp                    2496.580\n",
      "word_freq_will                  1495.270\n",
      "word_freq_your                  1223.100\n",
      "word_freq_hpl                   1204.400\n",
      "word_freq_re                    1159.140\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "Palabras en común en SPAM y No SPAM:\n",
      "{'word_freq_your', 'word_freq_you', 'word_freq_will', 'capital_run_length_total', 'capital_run_length_average', 'capital_run_length_longest'}\n"
     ]
    }
   ],
   "source": [
    "spam_word_frecuencies = column_sum.loc[1].sort_values(ascending=False)\n",
    "no_spam_word_frecuencies = column_sum.loc[0].sort_values(ascending=False)\n",
    "print('Spam:\\n', spam_word_frecuencies.head(10))\n",
    "print('No Spam:\\n', no_spam_word_frecuencies.head(10))\n",
    "\n",
    "common_words = set(spam_word_frecuencies.head(10).index).intersection(set(no_spam_word_frecuencies.head(10).index))\n",
    "print(\"\\nPalabras en común en SPAM y No SPAM:\")\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25203039",
   "metadata": {},
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "32a83687",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.3, random_state=50)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Lo transformamos en DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1142a5",
   "metadata": {},
   "source": [
    "## Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "f679f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predecir sobre el conjunto de prueba\n",
    "y_pred_naive_bayes_classifier = naive_bayes_classifier.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327652b",
   "metadata": {},
   "source": [
    "## Ejercicio 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "b6fcf320",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regresion_classifier = LogisticRegression()\n",
    "logistic_regresion_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred_logistic_regresion_classifier = logistic_regresion_classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec9d07",
   "metadata": {},
   "source": [
    "## Ejercicio 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "976b0302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusión Bayes Ingenuo:\n",
      "[[818  34]\n",
      " [ 95 434]]\n",
      "\n",
      "Matriz de confusión Regresion Logistica:\n",
      "[[811  41]\n",
      " [101 428]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el rendimiento Ing\n",
    "conf_matrix_naive_bayes_classifier = confusion_matrix(y_test, y_pred_naive_bayes_classifier)\n",
    "conf_matrix_logistic_regresion_classifier = confusion_matrix(y_test, y_pred_logistic_regresion_classifier)\n",
    "\n",
    "\n",
    "print(\"\\nMatriz de confusión Bayes Ingenuo:\")\n",
    "print(conf_matrix_naive_bayes_classifier)\n",
    "print(\"\\nMatriz de confusión Regresion Logistica:\")\n",
    "print(conf_matrix_logistic_regresion_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324364aa",
   "metadata": {},
   "source": [
    "## Ejercicio 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "e4feb3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del clasificador de Bayes ingenuo: 0.9065894279507604\n",
      "Exactitud del clasificador de Regresion Logistica: 0.9065894279507604\n"
     ]
    }
   ],
   "source": [
    "accuracy_naive_bayes_classifier = accuracy_score(y_test, y_pred_naive_bayes_classifier)\n",
    "accuracy_logistic_regresion_classifier = accuracy_score(y_test, y_pred_naive_bayes_classifier)\n",
    "\n",
    "\n",
    "print(\"Exactitud del clasificador de Bayes ingenuo:\", accuracy_naive_bayes_classifier)\n",
    "print(\"Exactitud del clasificador de Regresion Logistica:\", accuracy_logistic_regresion_classifier)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
